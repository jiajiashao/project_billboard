Project Billboard
=============================

A minimal, reproducible toolkit for perimeter‚Äëbillboard segmentation in sports video. It ships ready‚Äëto‚Äërun pipelines (SAM‚Äë2 or XMem) with optional auto‚Äëprompting (OWL‚ÄëViT, Moondream, or a fine‚Äëtuned YOLO11 seeder) and shot detection.

What‚Äôs reproducible:
- **Overlays & seed snapshots**: run on your own clips with no ground truth.
- **Metrics (IoU/BIoU/jitter)**: produced only when LabelMe GT masks exist under `sam2/data/gt_frames/<clip_id>/`.

Pick a pipeline under `sam2/` or `xmem/`, set up the env, and run one of the examples below. (XMem requires CUDA; SAM‚Äë2 seeding can run on CPU/MPS.)


Workflow
-------------------
<img src="appendent/workflow.png" alt="workflow" width="800"/>

Clone & Setup
-------------------
```bash
# clone & env
git clone https://github.com/jiajiashao/project_billboard.git
cd project_billboard/envs
conda env create -f envs/environment.yml
conda activate sam2 

# sanity: python and (optional) CUDA in THIS env
python -c "import sys; print(sys.version)"
python -c "import torch; print('cuda=', torch.cuda.is_available(), torch.version.cuda if torch.cuda.is_available() else None)"

# (Windows) if you hit cv2 import errors
pip install --upgrade opencv-python-headless

# ffmpeg present?
ffmpeg -version
```

This repo contains eight entry-point pipelines (SAM-2 and XMem variants) plus supporting prompt/shot-detection tooling. Paths below are repo‚Äërelative unless stated.

Environments
------------
- Conda/venv specs: `envs/environment.yml`, `envs/requirements.txt`.
- Create/activate an env that matches your runner (CUDA for XMem; CPU/MPS is fine for SAM-2 seeding only).
- Example setup: `conda env create -f envs/environment.yml` or `python -m venv .venv && pip install -r envs/requirements.txt`; then `conda activate <env>` or `source .venv/bin/activate`.

Data layout (relative to `--data-root` or `--root`)
---------------------------------------------------
For SAM-2 follow the below path(s) for data.
- `project_billboard/sam2/data/clips/<clip_id>.mp4` (input videos)
- `project_billboard/sam2/data/gt_frames/<clip_id>/frame_*.jpg` (pre-extracted frames, if present)
- `project_billboard/sam2/data/gt_frames/<clip_id>/frame_*.json` (LabelMe-style GT masks; used for fallback seeding/metrics)

For XMem follow the below path(s) for data.
- `project_billboard/xmem/data/clips/<clip_id>.mp4` (input videos)
- `project_billboard/xmem/data/gt_frames/<clip_id>/frame_*.jpg` (pre-extracted frames, if present)
- `project_billboard/xmem/data/gt_frames/<clip_id>/frame_*.json` (LabelMe-style GT masks; metrics)
- `project_billboard/xmem/data/gt_frames/<clip_id>/00000.png` (pre-extracted binary frames; used for seeding)

Additionally the prompts list is provided under project root project_billboard:
- Prompts list: `prompts_list.txt`

Models / weights
----------------
**Place or auto‚Äëdownload weights exactly to these paths:**

- **SAM‚Äë2** Pulled automatically by HuggingFace on first run: `facebook/sam2.1-hiera-tiny`

- **XMem checkpoint** (CUDA only)
Please download the XMem checkpoint from [Google Drive](https://) and place it inside: 
  - `project_billboard/xmem/model/xmem/saves/`

- **OWL‚ÄëViT** (auto‚Äëprompt)
  - Pulled automatically by HuggingFace on first run: `google/owlv2-base-patch16-ensemble`

- **Moondream** (auto‚Äëprompt)
  - Pulled on first run. If you hit Pillow conflicts, pin `pillow==10.4.*` in your env.

- **YOLO11 finetuned**
We provide the YOLO11n checkpoints finetuned on our dataset. 
  - `project_billboard/sam2/sam2_yolo11/weights/best.pt`
  - `project_billboard/xmem/xmem_yolo11/weights/best.pt`

Outputs
-------
- SAM-2: under `project_billboard/sam2/runs/<config>/<run_id>/` (overlay MP4, per-shot annotated JPGs, `re_prompts_*.csv`, `pilot_*.log`, summary CSVs).
- XMem: under `project_billboard/xmem/outputs/<run_id>/` (or `xmem/outputs/G_<clip>/runs/<run_id>/` for YOLO/XMem). You get masks PNGs, overlay MP4 (when XMem runs), `re_prompts_*.csv`, per-shot seed JPGs, pilot log, summary CSV.

Example: after a SAM‚Äë2 GT baseline on `clip_corner`, look under `sam2/runs/*/<run_id>/clip_corner/overlay.mp4` and `.../pilot_*.log`.



- Output example 1: Prompt boxes and overlay screenshot from run "clip_corner + OWL-ViT + SAM-2".
<img src="appendent/boxes_overlay.png" alt="boxes overlay" width="800"/>

- Output example 2: Overlay from run "clip_gentle + YOLO11n + SAM-2". The billboard is propagated with purple masks.
  
  ![Overlay video](appendent/overlay_clip_gentle_YOLO_sam2.gif)
  
  [üìπ Download original video (MP4)](appendent/overlay_clip_gentle_YOLO_sam2.mp4)




Known requirements / pitfalls
-----------------------------
- XMem **requires a CUDA-capable GPU**; on CPU/Mac it will exit early. Use `--skip-xmem` to debug seeds only.
- `cv2` import errors on some Python builds; reinstall OpenCV (`pip install opencv-python-headless`) in the active env.
- HuggingFace model downloads may warn about symlinks; use `HF_HUB_DISABLE_SYMLINKS_WARNING=1` if needed.

Entry points and key CLI flags
------------------------------

Each pipeline contains a prompter that generates box prompter and a segmentation model that produce masks.

<img src="appendent/pipelines.png" alt="pipelines" width="800"/>


- Prompts precedence: `--prompts` (CLI) > `--prompts-file` > built‚Äëin defaults.
- Path args differ: SAM‚Äë2 uses `--data-root`, XMem uses `--root`.
For each program, start by cd‚Äëing into the directory that contains its main.py, which keeps the command-line arguments to a minimum. For example:
 
`cd /project_billboard/xmem/xmem_yolo11`
`python main_yolo.py <required args>`

1) `../project_billboard/sam2/sam2_gt/main.py`
   - Required args: `--clips <id[,id2...]>`
   - Optional args:
     - `--data-root <path>` (default `./../data`)
     - `--weights <checkpoint_or_repo>` (default `facebook/sam2.1-hiera-tiny`)
     - `--runs-root <path>` (default `runs`)
     - `--device {cuda,cpu,mps}` (default `cuda`, auto-fallback if unavailable)
     - `--reseed` (flag; disabled by default)


2) `sam2/sam2_gt_shotdetector/main.py`
   - Args: same as above plus shot detection baked into pipeline.


3) `../project_billboard/sam2/sam2_moondream/main.py`
- Required args: `--prompts "<comma/semicolon-separated terms>"`, `--clips <id[,id2...]>`
- Optional args:
  - `--data-root <path>` (default `./../data`)
  - `--weights <checkpoint_or_repo>` (default `facebook/sam2.1-hiera-tiny`)
  - `--runs-root <path>` (default `runs`)
  - `--device {cuda,cpu,mps}` (default `cuda`, auto-fallback if unavailable)
  - `--auto-prompt` (flag; enabled by default)
  - `--moondream-model <hf_repo_or_path>` (default `vikhyatk/moondream2`)
  - `--moondream-device <device>` (default None; falls back to SAM-2 device logic)
  - `--moondream-threshold <float>` (default `0.10`)
  - `--prompts-file <path>` (alternatively provide prompts via file; overrides `--prompts` if set)
  - `--autoprompt-fallback {none}` (default `none`)


4) `../project_billboard/sam2/sam2_owlvit/main.py`
- Required args: `--prompts "<comma/semicolon-separated terms>"`, `--clips <id[,id2...]>`
- Optional args (same base options as the Moondream runner, with OWL-ViT-specific fields highlighted):
  - `--data-root <path>` (default `./../data`)
  - `--weights <checkpoint_or_repo>` (default `facebook/sam2.1-hiera-tiny`)
  - `--runs-root <path>` (default `runs`)
  - `--device {cuda,cpu,mps}` (default `cuda`, auto-fallback if unavailable)
  - `--auto-prompt` (flag; enabled by default)
  - `--owlvit-model <hf_repo_or_path>` (default `google/owlv2-base-patch16-ensemble`)
  - `--owlvit-device <device>` (default `None`; falls back to SAM-2 device logic)
  - `--owlvit-score-thr <float>` (default `0.15`)
  - `--prompts-file <path>` (alternative to `--prompts`; overrides it when provided)
  - `--autoprompt-fallback {none,gt}` (default `none`)


5) `../project_billboard/sam2/sam2_yolo11/main.py`
- Required args:
  - `--data <video_or_directory>` ‚Äî path to a single clip or a directory of clips (recurses for common video extensions). Example: `--data ../project_billboard/sam2/data/clips/clip_corner.mp4`
- Optional args:
  - `--out-dir <path>` (default `runs/yolo_sam2`) ‚Äì root folder for outputs; per-clip subfolders get a timestamp.
  - `--yolo-model <path>` (default `./weights/best.pt`) ‚Äì YOLO checkpoint for seeding boxes.
  - `--yolo-conf <float>` (default `0.20`) ‚Äì YOLO confidence threshold.
  - `--yolo-max-objects <int>` (default `3`) ‚Äì max YOLO detections per shot start.
  - `--sam2-weights <checkpoint_or_repo>` (default `facebook/sam2.1-hiera-tiny`) ‚Äì SAM-2 video weights.
  - `--input-width <int>` (default `1280`) ‚Äì resize width for decoded frames (aspect preserved).
  - `--device {cpu,cuda,mps}` (default `cuda`) ‚Äì compute device for YOLO + SAM-2.
  - `--stride <int>` (default `1`) ‚Äì sampling stride for metrics/visualization.
  - `--shot-mode {auto,single}` (default `auto`) ‚Äì auto-detect shot boundaries or treat the clip as a single shot.
  - `--gt-root <path>` (default `..\sam2\data\gt_frames`) ‚Äì ground-truth masks for metrics, when available.


6) `../project_billboard/xmem/xmem_gt/main_gt.py`
- Required args: *(none ‚Äî script runs with defaults; `clip_fast` is used when `--clip` is omitted)*
- Optional args:
  - `--root <path>` (default `./../`) ‚Äì project root containing `data/`, `model/`, `work/`, etc.
  - `--clip <id>` (default `clip_fast`) ‚Äì clip ID (expects `data/clips/<id>.mp4` and `data/seeds/<id>/00000.png`).
  - `--device {cuda,cpu}` (default `cuda`) ‚Äì XMem execution device.
  - `--frames <int>` (default `-1`) ‚Äì limit how many frames to extract (`-1` = all).
  - `--width <int>` ‚Äì resize width when extracting frames (keeps aspect).
  - `--stride <int>` (default `1`) ‚Äì metrics sampling stride.
  - `--run-id <name>` (default `xmem`) ‚Äì suffix for run directories/files.
  - `--run-notes` ‚Äì write a simple `RUN_NOTES.md` in the run folder.
  - `--run-only` ‚Äì skip metrics/summary; just prepare overlays and masks.
  - `--xmem-root <path>` ‚Äì override the XMem repo path (defaults to `<root>/model/xmem`).


7) `../project_billboard/xmem/xmem_owlvit/main_owlvit.py`
- Required args: `--clip <id or filename>` (defaults derive `.mp4` from the clip ID)
- Optional args:
  - `--root <path>` (default `./../`) ‚Äì project root containing `data/`, `model/`, etc.
  - `--width <int>` ‚Äì resize width when extracting frames (aspect preserved).
  - `--stride <int>` (default `1`) ‚Äì stride used for summary metrics.
  - `--run-id <name>` ‚Äì custom suffix for the run folder (defaults to `clip_<id>_<timestamp>`).
  - `--run-notes` ‚Äì write `RUN_NOTES.md` in the run directory.
  - `--run-only` ‚Äì skip metrics/summary CSVs (just prep masks/overlays).
  - `--shot-detect` ‚Äì enable per-shot processing.
  - `--shot-method {adaptive,content}` (default `adaptive`) ‚Äì strategy used when `--shot-detect` is on.
  - `--shot-min-seconds <float>` (default `1.0`) ‚Äì minimum shot duration for detection.
  - `--auto-prompt` (flag; enabled by default) ‚Äì turn OWL-ViT seeding on/off.
  - `--owlvit-model <hf_repo_or_path>` (default `google/owlv2-base-patch16-ensemble`).
  - `--owlvit-device <device>` ‚Äì override device just for OWL-ViT (defaults to SAM device logic).
  - `--owlvit-score-thr <float>` (default `0.15`).
  - `--prompts "<comma/semicolon separated terms>"` ‚Äì text prompts fed to OWL-ViT.
  - `--prompts-file <path>` ‚Äì alternative to inline prompts (first non-empty lines win).
  - `--autoprompt-fallback {gt,skip}` (default `skip`) ‚Äì use GT seeds when OWL-ViT yields none.
  - `--seed-erosion <int>` (default `1`) ‚Äì erosion iterations when rasterizing prompt boxes.
  - `--bbox-pad <int>` (default `6`) ‚Äì padding applied to boxes before seeding.
  - `--owlvit-debug` ‚Äì dump extra OWL-ViT diagnostics.
  - `--skip-xmem` ‚Äì only generate prompts/seeds; skip XMem propagation.


8) `../project_billboard/xmem/xmem_yolo11/main_yolo.py`
- Required args: `--clip <id>` (expects `data/clips/<id>.mp4`)
- Optional args:
  - `--root <path>` (default `./../`) ‚Äì XMem project root containing `data/`, `model/`, `work/`, etc.
  - `--device {cuda,cpu}` (default `cuda`)
  - `--width <int>` ‚Äì resize width when extracting frames (aspect preserved).
  - `--stride <int>` (default `1`)
  - `--run-id <name>` (default `xmem_yolo`)
  - `--run-notes` ‚Äì emit `RUN_NOTES.md` in the run directory.
  - `--run-only` ‚Äì skip metrics/summary generation.
  - `--shot-detect` ‚Äì enable per-shot processing.
  - `--shot-method {adaptive,content}` (default `adaptive`)
  - `--shot-min-seconds <float>` (default `1.0`)
  - `--auto-prompt` (flag; enabled by default) ‚Äì toggle YOLO seeding.
  - `--yolo-model <path>` (default `xmem_xmem_yolo11/weights/best.pt`) ‚Äì YOLO checkpoint.
  - `--yolo-conf <float>` (default `0.20`)
  - `--yolo-max-objects <int>` (default `3`)
  - `--yolo-imgsz <int>` ‚Äì inference resolution for YOLO (defaults to `--width` or `1280`).
  - `--autoprompt-fallback {gt,skip}` (default `skip`)
  - `--seed-erosion <int>` (default `1`)
  - `--yolo-debug` ‚Äì dump extra YOLO diagnostics.
  - `--skip-xmem` ‚Äì generate prompts/seeds only; skip XMem propagation.

Troubleshooting (common in this repo)
-------------------------------------
- **‚ÄúNo clips were processed‚Äù** ‚Üí check `--data-root/--root` and the `--clips/--clip` name exists under `data/clips/` (or `data/frames/` if running from frames).
- **CUDA available but script falls back to CPU** ‚Üí verify in the same env: `python -c "import torch; print(torch.cuda.is_available())"`. If `False`, you‚Äôre not on the right conda/venv.
- **HuggingFace symlink warning on Windows** ‚Üí set once and reopen shell: `setx HF_HUB_DISABLE_SYMLINKS_WARNING 1`.
- **OpenMP conflict during YOLO training** (`libomp` vs `libiomp5md`) ‚Üí temporary workaround: `setx KMP_DUPLICATE_LIB_OK TRUE` then restart shell.
- **`cv2` import errors** ‚Üí reinstall OpenCV in the active env: `pip install --upgrade opencv-python-headless`.

Tested / hardware
-----------------
- XMem eval: CUDA GPU required (fails fast if `torch.cuda.is_available()` is False).
- OWL-ViT / YOLO seeding: runs on CPU if CUDA is absent (but masks require CUDA).

Notes
-----
- Ensure video clips are present under `data/clips/` before running.
- Prompts: keep a reusable list in `prompts_list.txt` and pass via `--prompts-file` when supported.
- For seed-only debugging on non-CUDA hosts, use `--skip-xmem` in the XMem OWL-ViT/YOLO wrappers; inspect `re_prompts_*.csv` and `shot_XXX_seed.jpg`.

Acknowledgements
----------------
This repository assembles wrappers and helpers around the following open‚Äësource models and libraries. Please refer to their original licenses and documentation for usage terms and citations:

- **SAM-2 (Segment Anything Model 2)** ‚Äî Meta AI ‚Äî [facebookresearch/segment-anything-2](https://github.com/facebookresearch/segment-anything-2) ‚Äî License: **Apache-2.0** (plus third-party components noted in repo).
- **XMem** (long-term video object segmentation) ‚Äî [hkchengrex/XMem](https://github.com/hkchengrex/XMem) ‚Äî License: **MIT**.
- **OWL-ViT / OWL-V2** (open-vocabulary detection) ‚Äî Google Research Scenic ‚Äî [google-research/scenic/projects/owl_vit](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) ‚Äî License: **Apache-2.0**.
- **Moondream2** (lightweight VLM for box proposals) ‚Äî Hugging Face model card: [vikhyatk/moondream2](https://huggingface.co/vikhyatk/moondream2) ‚Äî License: **Apache-2.0**.
- **Ultralytics YOLO11** (first-frame seeding) ‚Äî [ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) ‚Äî License: **AGPL-3.0**.
- **PySceneDetect** (shot detection) ‚Äî [Breakthrough/PySceneDetect](https://github.com/Breakthrough/PySceneDetect) ‚Äî License: **BSD-3-Clause**.
- **OpenCV** (I/O & vision utils) ‚Äî [opencv/opencv](https://github.com/opencv/opencv) ‚Äî License: **Apache-2.0** (OpenCV ‚â•4.5).
- **FFmpeg** (video decoding/encoding) ‚Äî [ffmpeg.org](https://ffmpeg.org) ‚Äî License: **LGPL-2.1+**/**GPL-2.0+** (depending on build options).
- **Hugging Face Transformers** ‚Äî [huggingface/transformers](https://github.com/huggingface/transformers) ‚Äî License: **Apache-2.0**.
- **huggingface_hub** ‚Äî [huggingface/huggingface_hub](https://github.com/huggingface/huggingface_hub) ‚Äî License: **Apache-2.0**.
- **timm (PyTorch Image Models)** ‚Äî [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models) ‚Äî License: **Apache-2.0**.

We thank the respective authors and communities for making these resources available.

Appendent
----------------
Output analysis from 69 runs across the data video clips and configurations:

